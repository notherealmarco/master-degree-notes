#### Problems of biometric systems:
- wide intra-class variations
	- maybe different facial expression, different light, different view point...
- very small inter-class variations
	- two different person very similar (i.e. twins)

- possible spoofing attacks, in different moments
	![[Pasted image 20241002181936.png]]

- [non universality](LEZIONE2_Indici_di_prestazione.pdf#page=6&selection=0,10,0,26&color=yellow|LEZIONE2_Indici_di_prestazione, p.6)
	- e.g. people with no voice, people with cataract, people with poor fingerprints...

Most difficult traits to exploit:
- retina fundus
- behavioral traits (i.e. way of walking)
- handwriting

### [What to compare?](LEZIONE2_Indici_di_prestazione.pdf#page=8&selection=0,10,0,26&color=yellow|LEZIONE2_Indici_di_prestazione, p.8)
- **Sample
	- raw captured data
- **Hand-crafted features**
	- manually engineered by the data scientist and extracted from samples
	- can also be substituted with **embeddings**: features automatically extracted by deep architectures
- **Template**
	- collection of features extracted from the row data, examples:
		- a histogram representing the frequencies of relevant values in the image (e.g. greylevel values)
		- a vector of values each representing a relevant measure (e.g. Bertillon measures)
		- time series of acceleration values (one per axis)
		- a set of triplets as for relevant fingerprint points representing the coordinates of the points and the direction of the tangent to the ridge in that point.


> [!PDF|red] [[LEZIONE2_Indici_di_prestazione.pdf#page=8&selection=11,1,14,16&color=red|LEZIONE2_Indici_di_prestazione, p.8]]
> > Hand-crafted features
> 
> not the template of the entire biometric system.


### Comparing templates
- Euclidian distance
- Cosine similarity
	- cosine of the angle between two vectors
	- affected also by the direction of the vectors
- Pearson correlation (for histograms or sets of points)
	- statistical measure that evaluates the **linear relationship** between two variables. It tells you whether an increase or decrease in one variable tends to correspond with an increase or decrease in another, and how strong that relationship is (ChatGPT)
- Bhattacharyya distance (histograms)
	- measure of the similarity (or dissimilarity) between two probability distributions
	- the Bhattacharyya distance can compare feature distributions between two different classes (e.g., color histograms of objects)

> [!PDF|yellow] [[LEZIONE2_Indici_di_prestazione.pdf#page=10&selection=3,1,4,21&color=yellow|LEZIONE2_Indici_di_prestazione, p.10]]
> > (Pearson) Correlation
> 
> how signals are similar to eachother. Often used to compare fingerprints, by computing the correlation between two fingerprints.

For time series we have to address an issue: temporal sequences may vary in speed or timing, e.g. in two repetitions of a walking sequence, there might be differences in walking speed between repetitions, but the spatial path of limbs remain highly similar.
Another example could be audio recordings, same voice but different speed.

Dynamic Time Warping allows for "warping" of the time axis, meaning it can stretch or compress sections of the sequences to achieve the best possible alignment. This is useful when parts of one sequence are faster or slower than the corresponding parts in the other sequence.


![[Pasted image 20241002135922.png]]

each point is paired with the most convenient one. It's not necessarily that points corresponds to the same instant in time.

##### Comparing the results of submitting a template to a Deep Learning model
- if using deep learning we should use the architecture to extract the embeddings (for both gallery and probe templates): we can delete the classification layer in order to get the embeddings that the architecture would use for the final classification.
- mbeddings can be compard as they were vectors of hand-crafted features.

### Possible errors: verification
- Genuine Match (GM, GA): the claimed identity is true and subject is accepted
- False Rejection (FR, FNM, type I error): claimed identity is true but the subjet is rejected
- Genuine Reject (GR, GNM): an impostor is rejected
- False Acceptance (FA, FM, type II error): an impostor is accepted :/

It's important to define a good threshold.
If too high we will get a lot of false acceptance. If too low we will get a lot of false rejection!

When computing rates:
- **False Rejection Rate (FRR)** is the number of FR divided by the number of GM+FR.
	- in fact, GM and FR have the same denominator and sum up to 1.
- **False Acceptance Rate** is the number of FA divided by total number of impostor attempts (FA + GR)
- **Equal Error Rate** is the value at a specific threshold, where FAR and FRR are the same value.
- **Detection Error Trade-off:** a plot that shows the **trade-off** between the **FAR** and **FRR** at different threshold settings of a system
- **Receiving Operating Curve:** a plot that shows the True Positive Rate (TPR) (also called **Sensitivity**) against the False Positive Rate (FPR) (1 - Specificity) at various threshold settings.

Key Differences Between ROC and DET Curves:
- **ROC Curve**: Focuses on the **true positives** and **false positives**, showing the ability to discriminate between classes (genuine vs impostor).
- **DET Curve**: Focuses on the **false rejection rate (FRR)** and **false acceptance rate (FAR)**, helping to analyze trade-offs between security and usability in verification systems.

Two synthetic metrics for instance could be ERR and area below ROC curve.

(we might have more templates for the same person to address inter-class variation.
Of course templates should be different, not computed i.e. by frames of the same video, as some of them could be blurred and close frames are exactly the same!)

> [!PDF|yellow] [[LEZIONE2_Indici_di_prestazione.pdf#page=20&selection=119,0,119,4&color=yellow|LEZIONE2_Indici_di_prestazione, p.20]]
> > When
> 
> in false acceptance we can have two possible scenarios
> - pj does not belong to the gallery (most trivial)
> - pj belongs to an enrolled subject but the probe claimed another identity, not the real one.

What if ERR in two systems is the same, but the curves are different?

We can use ROC curve or DET curve.
For ROC, we can compute the area below the curve and use it as a metric, the higher the better.

#### Possible errors: identificaiton - open set
In an open set identification task, the system determines if the individual's biometric signature matches a signature of someone in the gallery.
The individual **does not make** and identity claim.
- More possible error situations, depending on the matcher and on the threshold
- A problem may occur if the system returns more possible candidates below the threshold. Who is the right one?
> [!PDF|yellow] [[LEZIONE2_Indici_di_prestazione.pdf#page=27&selection=0,8,9,8&color=yellow|LEZIONE2_Indici_di_prestazione, p.27]]
> > Possible errors: identification – open set
> 
> 

correct detect and identify rate = rate over which the correct individual has the identified score and so is identified correctly.

false alarm rate = rate over which unenrolled users are identified as another user in the db.

We compute that by testing the system with lots of probes belonging to set Pg if enrolled or set Pn if not.


We define
- rango(pj) = the position in the list where the first template for the correct identity is returned
- DIR (at rank k) (Detection and Identication Rate (at rank k)): the probability of correct identification at rank k (the correct subject is returned at position k)
- The rate between the number of individuals correctly recognized at rank k and the number of probes belonging to individuals in PG
- If identification does NOT happen at rank 1, we have a False Reject.
- FRR or more specifically FNIR (False Reject Rate or False Negative Identification Rate): the probability of false reject expressed as 1 - DIR (at rank 1)

- FAR or more specifically FPIR (False Acceptance Rate or False Positive Identification Rate) or False Alarm Rate (Watch List): the probability of false acceptance/alarm 
- The rate between the nuber of impostor recognized by error and the total number of impostors in PN


#### Closed set
We don't have thresholds!
The only possible error is that the correct identity does not appear at rank 1.

L’identificazione closed-set è un caso speciale in cui si assume che il soggetto
associato a ogni probe ha sicuramente una corrispondenza nella gallery. Questo
caso, seppur applicabile in pochi contesti, va comunque tenuto in considerazione.
Possiamo utilizzare le seguenti metriche:
- Cumulative Match Score a rango k ( **CMS(k)** ): probabilità di identificare un soggetto entro il rango k
- Cumulative Match Characteristic (**CMC**): curva che mostra per ogni rango k il valore $CMS(k)$
- Recognition Rate (**RR**): $CMS(1)$

## Organizzazione del dataset
Una cosa da tenere bene a mente è che in fase di sviluppo e test del modello noi abbiamo sempre a disposizione il “ground truth” (la verità) di ogni template, nel momento in cui invece un modello viene utilizzato poi in situazioni reali, questa informazione è ovviamente non disponibile.

Per una buona valutazione di un sistema è molto importante avere una buona suddivisione del dataset. Questa avviene su diversi livelli.

##### Suddivisione Training Set / Testing Set
L'obiettivo è quello di consentire una fase di training che possa garantire la generalità, deve quindi comprendere esempi molto differenziati tra loro.
Non tutti i sistemi richiedono necessariamente training in senso lato (ML), ma il training set può essere usato in ogni caso per impostare parametri da usare poi in fase di testing.
Si possono inserire nel TS soggetti non presenti nel TR per testare meglio la generalità del sistema.

##### Suddivisione probe vs gallery
Suddivisione effettuata per sample, condizione fondamentale $P ∩ G = ∅$
Nel gallery è buona norma inserire template catturati in condizioni controllate, in quanto nella realtà è la condizione più frequente.

##### Suddivisione probe set
Si sceglie quanti probe enrolled e quanti probe non enrolled selezionare
- $P = P_{G}∩P_{N}$ potrebbe condizionare i risultati di valutazione nel caso di identificazione open-set
- in identificazione closed-set, l'unica scelta possibile è chiaramente $P = P_{G}$
- in fase di verifica la scelta non condiziona i risultati

In generale, per evitare il più possibile di avere bias, bisognerebbe partizionare il dataset in diversi modi, ripetere l'evaluation e considerare le prestazioni medie.
## Probe vs All gallery
Possiamo costruire una matrice di distanze (es. distanza euclidea) tra coppie di template probe-gallery
- Per ogni coppia probe/gallery è possibile calcolare in anticipo una matrice di distanza probe-vs-all gallery, memorizzando le distanze tra coppie di template (probe template vs gallery template)
- La matrice può essere usata come strumento di valutazione per diversi tipi di applicazioni
- Ogni riga corrisponde a un'operazione di riconoscimento su un probe in input (con o senza claim di identità)
	- le colonne i gallery template
	- le righe rappresentano i probe template
- Si possono avere più di un sample per soggetto, e sarebbero rappresentati in colonne diverse della matrice
	- avere più sample diminuisce il FRR ma aumenta il FAR!

##### Verifica
- in caso di verifica, viene l'identità reale viene associata a quella claimed
- è importante considerare la claimed identity piuttosto che quella reale
- per una buona valutazione bisogna fare test con suddivisioni probe/gallery diverse
- per una buona valutazione bisogna fare test con diverse distribuzioni di probe genuine e probe di impostori

##### Identificazione open set
come prima, ma non abbiamo identity claim. Inoltre possono esistere probe non appartenenti a nessun'identità.
- per una buona valutazione bisogna fare test con suddivisioni probe/gallery diverse
- per una buona valutazione bisogna fare test con diverse distribuzioni di probe genuine e impostori

##### Identificazione closed set
come prima, ma non abbiamo impostori né threshold.
- per una buona valutazione bisogna fare test con suddivisioni probe/gallery diverse

## All-against-All
Metodo alternativo al precedente. Consiste nel calcolare una matrice di distanze tra tutti i possibili template, ognuno dei quali giocherà entrambi i ruoli di "probe" e "gallery template".

Pro:
- facile da programmare
- mette sotto stress il sistema (ci saranno più tentativi da impostore rispetto a quelli da utente genuino)

Contro:
- calcolo della matrice dispendioso
- impossibilità di calcolare diverse distribuzioni tra probe e gallery

Tutti gli algoritmi riportati di seguito hanno in comune i seguenti elementi:
- M = matrice delle distanze
- N = numero di soggetti
- S = numero di template per soggetto
- G = numero totale di sample = numero di righe/colonne di M ($|G|=SxN$)
- i = indice righe
- j = indice colonne
- $label(x)$ = vera identità di x

Ovviamente ad ogni riga escludo sempre un'operazione, perché non testo un template con se stesso.
##### Verifica, template singolo
 ogni riga è un set di |G| − 1 operazioni, con S − 1 test genuini e (N − 1) × (S) test da impostore.
```js
for each threshold t
	for each cell Mi,j with i != j
		if Mi,j <= t then
			if label(i) = label(j) then GA++
			else FA++
		else if label(i) = label(j) then FR++
		else GR++
GAR(t)=GA/TG //TG = |G| * (S-1) total genuine attempts
FAR(t)=FA/TI //TI = |G| * (N-1) * S total impostor attempts
FRR(t)=FR/TG
GRR(t)=GR/TI
```

##### Verifica, template multipli
invece di testare ogni coppia di template, testo un template nelle righe con tutto il gruppo di template che hanno la stessa identità nelle colonne, considerando però solo quello con distanza minore.
Per cui, ogni riga è un set di N operazioni (una per utente), con 1 test genuino e N − 1 test da impostore.

```js
for each threshold t
	for each row i
		for each group Mlabel of cells Mi,j with same label(j) excluding Mi,i
			diff = min(Mlabel)
			if diff <= t then
				if label(i) = label(Mlabel) then GA++
				else FA++
			else if label(i) = label(Mlabel) then FR++
			else GR++
GAR(t)=GA/TG
FAR(t)=FA/TI
FRR(t)=FR/TG
GRR(t)=GR/TI
```

##### Identificazione open-set, template multipli
ogni riga è un set di due operazioni, una da impostore e una da utente genuino.
```js
for each threshold t
	for each row i
		{Li,m| m=1 … |G|-1} =
		{Mi,j |j=1, … |G|} \ {Mi,i} ordered by increasing value
		// the identical element is excluded
		if Li,1 <= t then // potential accept
			if label(i) = label(Li,j) then
				DI(t, 1)++ // genuine case detected+identified

				// parallel impostor case:
				// jump the templates belonging to label(i) since i not in G
				find first Li,k such that label(Li,k) != label(i) and Li,k <= t
				if this k exists, then FA++
				// the first template != label(i) has a distance <= t
				else GR++ // impostor is correctly not detected
			else find first Li,k such that label(i) = label(Li,k) and Li,k <= t
			//if genuine yet not the first, look for higher ranks
				if this k exists, then
					DI(t, k)++ //end of genuine
				FA++ //impostor in parallel, distance below t but different label. No need to jump since the first label is not the «impostor»
		else GR++ // impostor case counted directly, FR computed through DIR

DIR(t, 1) = DI(t, 1)/TG
FRR(t) = 1 - DIR(t,1)
FAR(t) = FA/TI
GRR(t)=GR/TI
k=2 //higher ranks
while DI(t, k) != 0
	DIR(t, k) = (DI(t, k) / TG) + DIR(t, k-1) //we have to compute rates
	k++
```

##### Identificazione closed-set, template multipli
ogni riga è un'operazione da utente genuino, non ci sono impostori e non c'è threshold.
TA è il numero totale di attempts = |G|
```js
for each row i
	{Li,m| m=1 … |G|} = {Mi,j |j=1, …|G|} ordered by increasing value
	find the first Li,k such that label(i)=label(Li,k)
	CMS(k)++
	
CMS(1) = CMS(1) / TA
RR = CMS(1)

k=2
while k < |G| - 1
	CMS(k) = CMS(k) / TA + CMS(k - 1)
	k++
```