I fattori più importanti di un sistema biometrico sono l’accettabilità, l’affidabilità e l’accuratezza. l DNA, ad esempio, fornisce un’alta accuratezza e affidabilità ma una bassa accettabilità, in quanto il metodo di prelievo è sicuramente intrusivo. Le impronte digitali, invece, forniscono anch'esse buone prestazioni, ma possono spesso presentarsi in modo “parziale” e inoltre sono spesso associate ai ”criminali”. Il riconoscimento facciale invece è altamente accettato, in quanto siamo abituati a farci foto e a pubblicarle, ma l’accuratezza può diminuire drasticamente in casi non controllati.
Possibili problemi relativi a essa sono:
- Variazioni intrapersonali
- Similarità interpersonali
- PIE e A-PIE: posa, illuminazione ed espressione + invecchiamento
- Facilmente camuffabile: makeup, chirurgia plastica, occhiali, etc..

Steps:
- capture
- localizzazione
- cropping dei ROIs (regioni di interesse)
- normalizzazione
- feature extraction
- costruzione del template

#### Localizzazione della faccia
- Problema: data un'immagine o un video, rilevare la presenza di una o più facce e localizzarle nell'immagine
- Requisiti: deve funzionare indipendentemente da posizione, orientamento, dimensione, espressione, soggetti nell'immagine, illuminazione e sfondo.

##### Ci si può nascondere?
Secondo Adam Harvey, il punto chiave che i computer rilevano è il "nose bridge", o l'area tra gli occhi. Se si nascondono si può far credere al computer che non ci sia una faccia.

Approcci di diversa natura:
- **Basati su feature:** si individuano le feature principali di una faccia (ad esempio posizione occhi, posizione naso, etc..) e poi si possono verificare diverse proprietà di queste (es.: colore della pelle corretto o distanza tra naso e occhi entro una certa soglia)
- **Basati su immagine:** solitamente vengono utilizzati dei modelli di machine learning che imparano da immagini esemplificative.

> [!PDF|yellow] [[LEZIONE5_NEW_More about face localization.pdf#page=4&selection=56,0,61,0&color=yellow|LEZIONE5_NEW_More about face localization, p.4]]
> > pixels with the top 5 percent
> 
> white compensation using Luma

RGB is not a preceptually uniform space
- colors close to each other in RGB space may not be perceived as similar

RGB to YCrCb
![[Pasted image 20241023133231.png]]
Darker region may have dominant blue component, light region red.

Algoritmo A:
- variance-based segmentation
	- the simplest method is thresholding
		- we have an image I with L gray levels
		- $n_i$ is the number of pixels with gray level $i$
		- $p_i = \frac{n_i}{MxN}$ is the probability of gray level i
		- we divide pixel in two classes C0 and C1 by a gray level t
		- for each class we can compute mean and variance of gray level![[Pasted image 20241023135127.png]]
- connected components
	- skin tone pixels are segmented using local color variance![[Pasted image 20241023135925.png]]
	
Eye localization
The algorithm builds two different eye maps (chroma and luma)
	- **Chrominance map:** its creation relies on the observation that the region around eyes is characterized by high values of Cb and low values of Cr: $EyeMapC = \frac{1}{3}[(C_B^2)+(C_R^2)+(\frac{C_b}{C_r})]$
	- **Luminance map:** eyes usually contain both light and dark zones that can be highlighted by morphological operators (dilation and erosion with hemispheric structuring elements)![[Pasted image 20241023140250.png]]

Chroma map is enhanced by histogram equalization
The two maps are combined through AND operator
The resulting map undergoes dilation, masking and normalization to discard the other face regions and brighten eyes
Further operations allow to refine this map.

**Dilation**
The dilation operator takes two pieces of data as inputs. The first is the image which is to be dilated. The second is a (usually small) set of coordinate points known as a structuring element (also known as a kernel). It is this structuring element that determines the precise effect of the dilation on the input image.

**Erosion**
The erosion operator takes two pieces of data as inputs. The first is the image which is to be eroded. The second is a (usually small) set of coordinate points known as a structuring element (also known as a kernel). It is this structuring element that determines the precise effect of the erosion on the input image.

![[Pasted image 20241023141948.png]]

The algorithm analyzes all the triangles composed by two candidate eyes and a candidate mouth. Each triangle is verified by checking:
- Luma variations and average of the orientation gradient of the blobs containing eyes and mouth
- Geometry and orientation of the triangle
- Presence of a face contour around the triangle

#### Algoritmo B
Viola Jones rappresenta una vera e propria innovazione per quanta riguarda la localizzazione di una faccia all'interno di un’immagine. Essendo l’algoritmo basato su machine learning, il training di questo è avvenuto utilizzando un dataset personalizzato nel quale vengono etichettate immagini come positive nel caso in cui ci sia una faccia e come negative nel caso in cui non ve ne sia alcuna.

L'agoritmo image based usa un classifier inizialmente trainato con varie istanze delle classi da identificare (esempi positivi) e classi di immagini che non contengono nessun oggetto della classe (esempi negativi).

L'obiettivo del training è estrarre features dagli esempi e selezionare quelle più discriminative. Il modello costruito in modo incrementale e contiene le features.

L'algoritmo fa uso di:
-  **Ada-Boosting** per la selezione di feature: vengono creati diversi weak classifier, uno per feature, e tramite adaptive boosting riusciamo a creare uno strong classifier composto da un subset di weak-classifier.![[Pasted image 20241023144725.png]]

 AdaBoost è una tecnica di addestramento che ha lo scopo di apprendere la sequenza ottimale di classificatori deboli e i corrispondenti pesi.
 Richiede un insieme di pattern di training {(x1,y1),(x2,y2),...,(xN,yN)}, dove yi ∈{-1,+1} è l’etichetta della classe associata al pattern. Inoltre durante l’apprendimento è calcolata e aggiornata una distribuzione di pesi [w1,w2,...,wN] associati ai pattern di training, wi è associato al pattern (xi ,yi).
Dopo l’iterazione m, è assegnato ai pattern più difficili da classificare un peso w1(m) superiore, cosicché alla successiva iterazione m+1 tali pattern riceveranno un’attenzione maggiore.

un weak classifier è spesso un classifier lineare. Può essere comparato a una linea dritta.
![[Pasted image 20241024090856.png]]
in questo caso non va bene perché non tutti i rossi stanno dallo stesso lato. Nell'esempio è impossibile separare le due classi usando linee dritte.
p.s. non è un classifier a caso, è quello che in questo round ha il numero di errori minore.

![[Pasted image 20241024091151.png]]
Per trovare una classificazione che separa i sample problematici, incrementiamo i pesi.
![[Pasted image 20241024091235.png]]
questo classificatore separa correttamente i sample problematici
![[Pasted image 20241024091433.png]]![[Pasted image 20241024091446.png]]
![[Pasted image 20241024091511.png]]

##### Classifying faces with AdaBoost
Estraiamo feature rettangolari dalle immagini: le Haar features.
![[Pasted image 20241024092146.png]]

Quello che fa è calcolare: somma dell'intensità dei pixel che si trovano nell'area bianca) - somma dell'intensità dei pixel nell'area nera. Se il risultato dell’operazione è un numero grande allora vuol dire che con alta probabilità in quella porzione di immagine è presente la features identificata dal filtro (il filtro è uno dei quadrati sopra), dove ad esempio nel caso del B (nell'immagine sopra) sono angoli.

Per un immagine 24x24px, il numero di possibili rettangoli di features è 160'000!
Come si calcolano le Haar features? Possiamo usare AdaBoost per scegliere quali usare.

![[Pasted image 20241024092903.png]]
esempio molto stupido

![[Pasted image 20241024093000.png]]
esempio un po' meno stupido

Per ogni round di adaboost:
- proviamo ogni filtro rettangolare su ogni esempio
- scegliamo la threshold migliore per ogni filtro
- scegliamo la miglior combo filtro/threshold
- ricalcoliamo i pesi
Complessità computazionale: O(MNT)
- M filters
- N examples
- T thresholds

Le rectangular features possono essere valutate attraverso immagini integral il quale nome viene dato, in ambito computer vision, ad un algoritmo con annessa struttura dati chiamata Summed-Area table, la quale ci consente di calcolare l’area di una sottomatrice in tempo costante.
L'immagine integrale in posizione (x,y) è la somma del valore dei pixel sopra e a sinistra di (x,y):
$$II(x,y)=\sum_{x'<=x,y'<=y}I(x',y')$$
Usando integral image è possibile calcolare la somma dei valori dei pixel in ogni rettangolo:
![[Pasted image 20241024094223.png]]

Il singolo weak classifier dipende dai parametri $z_k$ (feature) e $t_k$ (threshold):
- per ogni feature scegliamo un valore di threshold che minimizza l'errore di classificazione
- si sceglie poi la feature con meno errore

Un solo classificatore robusto, per quanto elimini una grande porzione di sottofinestre che non contengono facce, non soddisfa i requisiti di applicazioni. Una possibile soluzione consiste nell'impiego di classificatori in cascata (cascade classifier), via via più complessi:
![[Pasted image 20241024095704.png]]

un classificatore per 1 sola feature riesce a passare al secondo stadio la quasi totalità dei volti esistenti (circa 100%) mentre scarta al contempo il 50% dei falsi volti.
Un classificatore per 5 feature raggiunge quasi il 100% di detection rate e il 40% di false positive rate (20% cumulativo) usando i dati dello stadio precedente.
Un classificatore per 20 feature raggiunge quasi il 100% di detection rate con 10% di false positive rate (2% cumulativo).

La localizzazione dei volti avviene analizzando sottofinestre consecutive (sovrapposte)
dell’immagine in input e valutando per ciascuna se appartiene alla classe dei volti:
![[Pasted image 20241024100002.png]]

#### Valutazione della localizzazione
- **Falsi positivi:** percentuale di finestre classificate come volto che in realtà non lo contengono
- **Facce non localizzate:** percentuale di volti che non sono stati individuati
- **C-ERROR** o Errore di localizzazione: distanza euclidea tra il reale centro della faccia e quello ipotizzato dal sistema, normalizzato rispetto alla somma degli assi dell’ellisse contenente il volto.